<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title> ACCV2016</title>

    <!-- Bootstrap Core CSS -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/1-col-portfolio.css" rel="stylesheet">
    
     <!-- sidebar menu  downloaded from: http://startbootstrap.com/template-overviews/simple-sidebar/ -->
    <link href="../css/simple-sidebar.css" rel="stylesheet">
    
    

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
           <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="http://sahasuman.bitbucket.org">Home</a>
                    </li>
                   
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">

        <!-- Page Heading -->
         <!-- Page Heading -->
        <div class="row">
            <div class="col-lg-12">
                <h1 class="page-header">
                <a name="accv2016-home">
                <!--<br><br><br>-->
                Towards real-time action detection and prediction  
                </a>
                </h1>
            </div>
        </div>
        <!-- /.row -->
	
 
    
        <!-- Project One -->
        <div class="row">        
        <!-- sidebar navigation meny -->
	  <div id="sidebar-wrapper">
            <ul class="sidebar-nav">   
	      <li>  <a href="#accv2016-home">Project Home</a>  </li>
	      <li>  <a href="#abstract">Abstract</a>  </li>
	      <li>  <a href="#publication">Related Publication</a>  </li>
	       <li>  <a href="#overview">Overview of the Approach</a>  </li>	      
	      <!--
	      <li>  <a target="_blank" href="https://bitbucket.org/sahasuman/bmvc2016_code">Code</a>  </li>
	      <li>  <a href="#video">Video</a> </li>  
	      -->
	      <li>  <a href="#qty-res"> Offline Quantitative Results</a> </li>
	      <li>  <a href="#qty-res-2"> Online Quantitative Results</a> </li>
              <li>  <a href="#visual-results">Visual Results</a>  </li>
              <li>  <a href="#sup1"> RPN Recall-vs-IoU </a> </li>
              <li>  <a href="#sup2">  Ablation Study </a> </li>
              <li>  <a href="#sup3">  Label smoothing and mAP </a> </li>
              <li>  <a href="#sup4">  Train/Test Computing Time </a> </li>
                       
            </ul>
	  </div>
	   <!-- intro teaser image -->
            <div class="col-md-12">		  
                    <img class="img-responsive" src="imgs/intro-1.png" alt="">  
                    <p>
              Action prediction and online action localisation in a `kick-ball' test video taken from J-HMDB-21 dataset.  
(a) to (d): A 3D volumetric view of the video with selected frames.
 At any given time, a certain portion (%) of the entire video is observed by the system, 
and the detection boxes (in red) are linked up to incrementally build an online space-time action tube.
(e) A frame-level 2D view of the same video shows the predicted action label at different time intervals.
 The Ground-truth bounding boxes are depicted in green and the detection boxes are in red. </p>
                
            </div>
         </div>
        <!-- /.row -->
        
      <!-- this is for video but we don't have video for accv2016 submission-->  
      <!--  <hr>
         
        <div>        
             <h3>
             <a name="video">
             <br><br><br>
             Video
             </a>
             </h3>
             <iframe width="420" height="315" src="https://youtu.be/hNTXR3S6AQY"> </iframe> 
        </div>-->
      
        <hr>

        <!-- Project Two -->
        <div class="row">            
            <div class="col-md-12">
                <h3>
                <a name="abstract">
                <br><br><br>
                Abstract
                </a>
                </h3>
                <p style="text-align:justify;">               
              In this work we propose a new efficient deep learning approach to spatio-temporal localisation (detection) 
and classification of multiple concurrent actions within temporally untrimmed videos, 
which is suitable for online action label prediction and localisation in real time.
Our framework is composed of three stages.
In stage 1, two end-to-end trainable YOLO convolutional neural networks are employed to regress and classify detection boxes 
in each video frame potentially containing an action of interest, one from optical flow and one from RBG images.
In stage 2, appearance and motion cues are combined by merging the detection boxes and  classification scores generated by the two networks.
In stage 3, sequences of detection boxes most likely to be associated with a single action instance, called {action tubes}, 
are constructed incrementally and efficiently by dynamic programming, allowing the system to
perform early action class prediction and spatial localisation in real time.
We demonstrate the performance of our algorithm on the challenging UCF101 and J-HMDB-21 datasets in both the offline and online settings, 
achieving new state-of-the-art results and significantly lower test time detection latency across the board.
		
                </p>
             </div>
        </div>
         
         <hr>
         
          <div class="row">            
            <div class="col-md-12">
               
                <h3>
                <a name="publication">
                <br><br><br>
                Related Publication:
                </a>
                </h3>
                 <h4>
                Towards real-time action detection and prediction, Asian Conference on Computer Vision (ACCV), 2016.                  
                </h4>
                <h4>        
                 <a href="http://gurkirt.github.io/">Gurkirt Singh</a>,   
                 <a href="http://sahasuman.bitbucekt.org"> Suman Saha</a>,
                 <a href="http://cms.brookes.ac.uk/staff/FabioCuzzolin/">Fabio Cuzzlion</a>
                </h4>                
                <a href="#"> bibtex </a> |
                <a href="#accv2016-home"> project page </a> | 
                <a href="#"> paper </a> |
                <a href="#"> suplementary material </a> |
                <a href="#" name="code"> code </a> 
                <!-- <a href="#video"> video </a> -->
            </div>
        </div>
        
          <hr>
         
         <div class="row">            
            <div class="col-md-12">  
            <a name="overview">
            <br><br><br>
             <h3>Overview of the action detection pipeline with near realtime detection speed</h3>
             </a>
              <img class="img-responsive" src="imgs/overview.png" alt="">
              <p>
             At test time,
    (a) RGB and flow video frames are passed to
    (b) two separate YOLO detection networks.
    (c) Each network outputs detection boxes and class-specific confidence scores.
    (d) Appearence and flow detections are fused and  
    linked up to generate class-specific action paths spanning the whole video 
    for either (e) online action prediction and localisation
    or (f) offline spatiotemporal action localisation.
    (g) In the latter case action paths are temporally trimmed to form the final action tubes.
              </p>
              <br>
              <img class="img-responsive" src="imgs/det-net.png" alt="">
              <p>
              Integrated detection network (YOLO Network [*] ): (a) an input video frame is fed to the network; 
(b) input frame is divided into a grid of S x S cells;
(c) each cell in the grid predicts B bounding boxes and their actionness scores;
(d) the final outputs are the predicted bounding boxes and their class-specific confidence scores.
              </p>
              <br>
              <img class="img-responsive" src="imgs/det-net-2.png" alt="">
              <p>
              Block-diagram of the detection network (YOLO Network [*] ) architecture, 
showing its 24 convolutional layers followed by 2 fully connected layers.
(*) The final output layer's dimension is 7 x 7 x 31 for J-HMDB-21, 7 x 7 x 34 for UCF-101.
              </p>
              <small>
              [*] You only look once: Unified, real-time object detection. arXiv preprint arXiv:1506.02640 (2015) Redmon, J., Divvala, S., Girshick, R., Farhadi, A.
              </small>
              
	    </div>
	</div>	 
	
        <hr>
         <div>
         <h3>
         <a name="qty-res"> 
         <br><br><br> 
         Offline Quantitative action detection results: 
         </a>         
         </h3>
         </div>
         <br>        
	  <div class="row">            
            <div class="col-md-12">  
            <h4>
              Quantitative action localisation results (mAP) on the J-HMDB-21 dataset:
              </h4>
              <img class="img-responsive" src="imgs/qty-res-1.png" alt="">   
               <h4>
               <br>
              Quantitative action detection results (mAP) on the UCF-101 dataset:
              </h4>
              <img class="img-responsive" src="imgs/qty-res-2.png" alt="">   
              
	  </div>
	</div>
	
	 <hr>
         <div>
         <h3>
         <a name="qty-res-2"> 
         <br><br><br> 
         Online Quantitative action detection results: 
         </a>         
         </h3>
         </div>
         <br>        
	  <div class="row">            
            <div class="col-md-12">  
            <h4>
              Action prediction and online action localisation results on J-HMDB-21:
              </h4>
              <img class="img-responsive" src="imgs/qty-res-3.png" alt="">   
               <p>
               Early action prediction and online localisation results on JHMDB:
(a) prediction accuracy curves versus video observation %;
(b) and (c) online action localisation curves for:
(b) mean Average Precision (mAP);
(c) AUC at different IoU thresholds (10%=0.1, 40%=0.4 and 60%=0.6).
(d) action localisation in the offline setting.  
		  </p>
	      </div>
	  </div>
	
	
	       
        <div>
        <br>
         <p> Note that the reference numbers are in line with our <a href="#publication"> ACCV2016 paper.</a> <p>
        </div>
        <hr>
        <div> 
        <h3>
        <a name="visual-results"> 
        <br><br><br>        
        Visual results        
        </a>
        </h3>
        </div>
         
         <div class="row">            
            <div class="col-md-12">            
             <h4>Action detection/localisation results on UCF101 dataset</h4>
              <img class="img-responsive" src="imgs/ucf101-1.png" alt="">  
              <p>
              Ground-truth boxes are in green, detection boxes in red. The top row shows correct detections, the bottom one contains examples of more mixed results. In the last frame, 3 out of 4 
	      `Fencing' instances are nevertheless correctly detected.
              </p>
	    </div>
        </div>
        
         <hr>
         
          <div class="row">            
            <div class="col-md-12">            
             <h4>Sample space-time action localisation results on JHMDB-21 dataset</h4>
              <img class="img-responsive" src="imgs/jhmdb-21.png" alt="">  
              <p>
              Left-most three frames: accurate detection examples. Right-most three frames: mis-detection examples.
              </p>
	    </div>
        </div>
        
         <hr>
         
           <div class="row">            
            <div class="col-md-12">            
             <h4>Sample space-time action localisation results on LIRIS-HARL dataset</h4>
              <img class="img-responsive" src="imgs/liris.png" alt="">  
              <p>
              Frames from the space-time action detection results on LIRIS-HARL, some of which include single actions involving more than one person like ‘handshaking’ and ‘discussion’.
              Left-most 374 three frames: accurate detection examples. Right-most three frames: mis-detection examples.
              </p>
	    </div>
        </div>
        
         <hr>
         
         <div class="row">            
            <div class="col-md-12">            
             <h4>Sample spatio-temporal localisation results on UCF-101</h4>
              <img class="img-responsive" src="imgs/ucf101-supl-res.png" alt="">  
              <p>
              Each row represents a  UCF-101 test video clip. Ground-truth bounding boxes are in green, detection boxes in red.
              </p>
	    </div>
        </div>
        
        <hr>
        
        <div class="row">            
            <div class="col-md-12">            
             <h3>
             <a name="sup1">
             <br><br><br>
             Selective Search vs RPN action proposals
             </a>
             </h3>
              <img class="img-responsive" src="imgs/sup-1.png" alt="">  
              <p>
             Performance comparison between Selective Search (SS) and RPN-based region proposals on four groups of action classes (vertical columns) in UCF-101. Top row: recall vs. IoU curve for SS. 
Bottom row: results for RPN-based region proposals.
             </p>
	    </div>
        </div>
        
        <hr>
        
         <div class="row">            
            <div class="col-md-12">            
             <h3>
             <a name="sup2">
             <br><br><br>
              An ablation study of the spatio-temporal detection results (mAP) on UCF-101.
             </a>
             </h3>
              <img class="img-responsive" src="imgs/sup-2.png" alt="">                
	    </div>
        </div>
        
        <hr>
        
         <div class="row">            
            <div class="col-md-12">            
             <h3>
             <a name="sup3">
             <br><br><br>
              Impact of label smoothing on detection performance (mAP) on UCF-101.
             </a>
             </h3>
              <img class="img-responsive" src="imgs/sup-3.png" alt="">  
              
	    </div>
        </div>
        
        <hr>
        
         <div class="row">            
            <div class="col-md-12">            
             <h3>
             <a name="sup4">
             <br><br><br>
              Train/Test Computing Time
             </a>
             </h3>
              <img class="img-responsive" src="imgs/sup-4.png" alt="">  
               <p> Note: that the reference numbers are in line with our <a href="#publication"> BMVC2016 suplementary material.</a> <p>
        
	    </div>
        </div>
      <!-- Footer --> 
     <br><br><br><br> 
     <hr>
     <hr>
    
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>
                    Webpage designed by <a href="http://getbootstrap.com/2.3.2/index.html"> Bootstrap </a>.
                    </p>
                </div>
            </div>
            <!-- /.row -->
        </footer>

    </div>
    <!-- /.container -->
      
    
     <div style="text-align:center;"><script type="text/javascript" src="http://services.webestools.com/cpt_visitors/40087-10-9.js"></script></div>
    
    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    
</body>

</html>
