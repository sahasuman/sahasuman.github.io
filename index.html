<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Suman Saha</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet"> 
    <!-- <link href="http://sots.brookes.ac.uk/~14034797/css/bootstrap.min.css" rel="stylesheet"> --> <!-- this for sots.brookes.ac.uk --> 

    <!-- Custom CSS -->
    <link href="css/1-col-portfolio.css" rel="stylesheet">
    <!-- <link href="http://sots.brookes.ac.uk/~14034797/css/1-col-portfolio.css" rel="stylesheet"> --> <!-- this for sots.brookes.ac.uk -->
    
    
     <!-- sidebar menu  downloaded from: http://startbootstrap.com/template-overviews/simple-sidebar/ -->
    <link href="css/simple-sidebar.css" rel="stylesheet">
    <!-- <link href="http://sots.brookes.ac.uk/~14034797/css/simple-sidebar.css" rel="stylesheet"> --> <!-- this for sots.brookes.ac.uk -->

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
         <!--   <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#">Start Bootstrap</a>
            </div>-->
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="#">Home</a>
                    </li>
                    <li>
                        <a href="#publication">Publications</a>
                    </li>
                    <li>
                        <a href="CV/SumanSaha-CV.pdf">Bio/CV</a>  <!-- CV PDF FILE PATH -->
                    </li>
                    <li>
                        <a href="#awards">Awards</a>
                    </li>
                    
                     <li>
                        <a href="#activity">Activities</a>
                    </li>
                    
                    <li>
			<a href="#robot">Extracurricular Activities</a>
                    
                    <li>
                     <li>
			<a href="#gallery">Photo/Video Gallery</a>
                    
                    <li>
                        <a href="#">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">
      
      
      <!-- sidebar navigation meny -->
	<!--  <div id="sidebar-wrapper">
            <ul class="sidebar-nav">   
	      <li>  <a href="#home">Home</a>  </li>
	      <li>  <a href="#robot"> Extracurricular Activities </a>  </li>
	                             
            </ul>
	  </div>-->
	  
	  
	  
        <!-- Page Heading -->
        <div class="row">
            <div class="col-md-2">
                <img class="img-responsive" src="imgs/myImg.jpg" alt="">
            </div>
            <div class="col-md-10">
                <h4><a name="home">Suman Saha</a></h4>
                <p align="justify">
I am a PhD student at the <a href="http://cct.brookes.ac.uk/research/index.html" target="_blank">Department of Computing and Communication Technologies</a>
in <a href="https://www.brookes.ac.uk/homepage/" target="_blank">Oxford Brookes University</a>.
My Director of Studies and first supervisor is   <a href="http://cms.brookes.ac.uk/staff/FabioCuzzolin/" target="_blank">Professor Fabio Cuzzolin</a>. 
<a href="https://www.brookes.ac.uk/profiles/staff/nigel-crook/" target="_blank">Professor Nigel Crook</a> and
<a href="http://cct.brookes.ac.uk/staff/tjeerdoldescheper.html" target="_blank">Dr Tjeerd Olde Scheper</a> are my two other PhD supervisors.
I am an active member of the <a href="http://cct.brookes.ac.uk/research/isec/artificial-intelligence/index.html" target="_blank">
Artificial Intelligence and Vision Research Group</a> led by Professor Fabio Cuzzolin.
I consider myself fortunate to have an opportunity to work closely with the world renowned
<a target="_blank" href="http://www.robots.ox.ac.uk/~tvg/">Torr Vision Group (TVG)</a>
in the Department of Engineering Science at University of Oxford. More specifically, I work with my PhD guide
<a href="https://sites.google.com/site/mikesapi/" target="_blank">Dr Michael Sapienza</a>
and  <a href="http://www.robots.ox.ac.uk/~phst/" target="_blank">Professor Philip H. S. Torr.</a> who is the 
founder of <b>TVG</b>. 
My research areas are Computer Vision and Machine Learning and I am interested in applying
<a href="http://deeplearning.net/" target="_blank"> Deep Learning</a>  techniques in solving challenging Computer Vision problems, for example, the
<a href="http://sahasuman.bitbucket.io/bmvc2016/" target="_blank">spatio-temporal human action localisation</a>
problem in videos.
We have recently proposed a novel 
<a href="http://sahasuman.bitbucket.io/bmvc2016/" target="_blank">Deep Learning based spatio-temporal action localisation framework</a> which 
has shown superior performance and become the state-of-the-art. My email id is: suman.saha-2014@brookes.ac.uk. <a target="_blank" href="aboutme.html">Click here to read more about me.</a>
                </p>
            </div>
        </div>
        <!-- /.row -->        
       
	<div class="row"> 
                <h3>News:</h3>               
                <ul> 
                <li>
                 <span style="color:red;"> <b> *NEW* </b> </span>
                 April - July 2017: Worked as a research lab associate at Disney Research Zürich, Switzerland.
                 </li>
                 <li>  
                 <span style="color:red;"> <b> *NEW* </b> </span>
        17th July 2017: The following two papers get accepted in <b> ICCV 2017 </b> (International Conference on Computer Vision), Venice, Italy, October 22-29, 2017.<br>
        Paper-1: AMTnet: Action-Micro-Tube regression by end-to-end trainable deep architecture,
        <a href="https://arxiv.org/abs/1704.04952" target="_blank"> arXiv. </a> <br>
        Paper-2: Online Real-time Multiple Spatiotemporal Action Localisation and Prediction.
        <a href="https://arxiv.org/abs/1611.08563" target="_blank"> arXiv. </a>         
                </li>
                <li> 
                <span style="color:red;"> <b> *NEW* </b> </span>
                16th February 2017: following paper has been accepted in the Journal of Gait & Posture 54 (2017): 127-132. <br>                
                Metric learning for Parkinsonian identification from IMU gait measurements.
                <a href="http://www.sciencedirect.com/science/article/pii/S0966636217300462" target="_blank"> paper link </a>     
                
               </li>
                
                <li> 1st August, 2016: Presented a poster on 
                <a href="https://drive.google.com/open?id=0B56r0lkehn6WZHZqdmtiVFhMWUE" target="_blank">
                Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos</a>
                in the
                <a href="http://www.robots.ox.ac.uk/~nsid/muri/" target="_blank">
                MURI Meeting</a>
                at
                <a href="http://www.stcatz.ox.ac.uk/About-Us" target="_blank">St. Catherine’s College.</a>
                <li>15th July, 2016: Our paper "Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos" has been accepted by BMVC 2016.</li>
                <!-- 17th July to 23rd July 2016: Our accepted BMVC 2016 work is presented in the poster session at <a href="http://svg.dmi.unict.it/icvss2016/">International Computer Vision Summer 
		School(ICVSS) 2016 summer school</a>.-->
		 <li>12th July to 18th July 2015: Presented a <a target="_blank" href="icvss2015/poster.pdf"> poster 
                on "Online Human Action Localisation based on Appearance and Motion Cues" <a> at 
                <a target="_blank" href="http://svg.dmi.unict.it/icvss2015/">
                (ICVSS) 2015</a>, held in Sicily, Italy. </li>
                <li>12th July to 18th July 2015: Our group won the reading group competition at ICVSS 2015 Summer School. </li>
                <li>12th July to 18th July 2015: Successfully passed ICVSS 2015 Examination and received the 
                <a target="_blank" href="icvss2015/ExaminationCertificate-102-correct-date.pdf"> certification</a>.</li>
                <li>26th - 28th September, 2014: Attended the Third International Conference on Belief Functions at St Hugs College, University of Oxford, UK.</li>
                <li> 2014: Our paper 
                <a target="_blank" href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7024382">
                A real-time monocular vision-based frontal obstacle detection and avoidance for low cost UAVs in GPS denied environment                
                </a>
                is accepted in Aerospace Electronics and Remote Sensing Technology (ICARES), 2014 IEEE International Conference on. </li>
                
                
                </ul>
                
        </div>
        
	<hr>
	<div class="row"> 
                <h3>Projects:</h3>
        </div>
        
        <br>
        
        <!--ICCV 2017 AMTNet paper-->
        <div class="row">
            <div class="col-md-6">
                <a href="#">
                    <img class="img-responsive" src="iccv2017/amt_net/imgs/iccv2017_amtnet.png" alt="">
                </a>
            </div>
            <div class="col-md-6">
                <h5>                
                 <a href="#">
                AMTnet: Action-Micro-Tube regression by end-to-end trainable deep architecture.                 
                </a>               
                </h5>
                <h5>
                 <a href="#"> Suman Saha</a>,
                 <a href="http://gurkirt.github.io/">Gurkirt Singh</a>,  
                 <a href="http://cms.brookes.ac.uk/staff/FabioCuzzolin/">Fabio Cuzzolin.</a>
                </h5>
                <p> 
                 <span style="color:red;"> <b> *NEW* </b> </span>
                 Accepted in <b> ICCV 2017 </b> (International Conference on Computer Vision), Venice, Italy, October 22-29, 2017.
                 </p>
                <p align="justify">                
                Dominant approaches to action detection can only provide sub-optimal solutions to the problem, as they rely on seeking frame-level detections, to later compose them into ‘action tubes’ in a post-processing step. With this paper we radically depart from current practice, and take a first step towards the design and implementation of a deep network architecture able to classify and regress whole video subsets, so providing a truly optimal solution of the action detection problem.
                </p>
                <a href="#" target="_blank">
                Project page
                </a> |
                <a href="https://arxiv.org/abs/1704.04952" target="_blank">
                arXiv
                </a> | 
                <a href="#" target="_blank">Poster (will be updated soon...)
                </a>
            </div>
       </div>
        <!-- /.row -->
        
         <hr>
         
         
         <br>
        
        <!--ICCV 2017 online det paper-->
        <div class="row">
            <div class="col-md-6">
                <a href="#">
                    <img class="img-responsive" src="iccv2017/online_det/imgs/iccv2017_online_det.png" alt="">
                </a>
            </div>
            <div class="col-md-6">
                <h5>                
                 <a href="#">
                Online Real-time Multiple Spatiotemporal Action Localisation and Prediction.                 
                </a>                
                </h5>               
                <h5>                 
                 <a href="http://gurkirt.github.io/">Gurkirt Singh</a>, 
                 <a href="#"> Suman Saha</a>,
                 <a href="https://sites.google.com/site/mikesapi/">Michael Sapienza</a>,
                 <a href="http://www.robots.ox.ac.uk/~tvg/"> Philip H. S. Torr</a>,
                 <a href="http://cms.brookes.ac.uk/staff/FabioCuzzolin/">Fabio Cuzzolin.</a>
                </h5>
                 <p> 
                 <span style="color:red;"> <b> *NEW* </b> </span>
                 Accepted in <b> ICCV 2017 </b> (International Conference on Computer Vision), Venice, Italy, October 22-29, 2017.
                 </p>
                <p align="justify">                
                We present a deep-learning framework for real-time multiple spatio-temporal (S/T) action localisation, classification and early prediction. Current state-of-the-art approaches work offline, and are too slow be useful in realworld settings. To overcome their limitations we introduce two major developments. Firstly, we adopt real-time SSD (Single Shot MultiBox Detector) convolutional neural networks to regress and classify detection boxes in each video frame potentially containing an action of interest. Secondly,
                we design an original and efficient online algorithm to incrementally construct and label ‘action tubes’ from the SSD frame level detections.
                </p>
                <a href="#" target="_blank">
                Project page
                </a> |
                <a href="https://arxiv.org/abs/1611.08563" target="_blank">
                arXiv
                </a> | 
                <a href="#" target="_blank">Poster (will be updated soon...)
                </a>
            </div>
       </div>
        <!-- /.row -->
        
         <hr>
         
         
          <!--Gait and Posture journal paper paper-->
        <div class="row">
            <div class="col-md-6">
                <a href="#">
                    <img class="img-responsive" src="gait_and_posture/imgs/gp.png" alt="">
                </a>
            </div>
            <div class="col-md-6">
                <h5> 
                Metric learning for Parkinsonian identification from IMU gait measurements.  
                Gait & Posture, Volume 54, May 2017, Pages 127-132.
                                
                </h5>               
                <h5> 
                                 
                 <a href="http://cms.brookes.ac.uk/staff/FabioCuzzolin/">Fabio Cuzzolin</a>,              
                 <a href="https://sites.google.com/site/mikesapi/">Michael Sapienza</a>,
                 <a href='#'> Patrick Esser</a>,
                 <a href="#"> Suman Saha</a>,
                 <a href="#"> Miss Marloes Franssen</a>,                  
                 <a href="#"> Johnny Collett </a>,
                 <a href="#"> Helen Dawes </a> 
                </h5>
                 <p> 
                 <span style="color:red;"> <b> *NEW* </b> </span>
                                 
                 </p>
                <p align="justify">                   
                 In this paper we propose a novel framework in which IMU gait measurement sequences sampled during a 10 m walk are first encoded as hidden Markov models (HMMs) to extract their dynamics and provide a fixed-length representation. Given sufficient training samples, the distance between HMMs which optimises classification performance is learned and employed in a classical Nearest Neighbour classifier. Our tests demonstrate how this technique achieves accuracy of 85.51% over a 156 people with Parkinson's with a representative range of severity and 424 typically developed adults, which is the top performance achieved so far over a cohort of such size, based on single measurement outcomes. The method displays the potential for further improvement and a wider application to distinguish other conditions.
                </p>
                <a href="#" target="_blank">
                Project page
                </a> |
                <a href="http://www.sciencedirect.com/science/article/pii/S0966636217300462" target="_blank">
                Online paper link
                </a> 
               
            </div>
       </div>
        <!-- /.row -->
        
         <hr>
         
        
                
        <!--BMVC 2016 paper-->
        <div class="row">
            <div class="col-md-6">
                <a href="#">
                    <img class="img-responsive" src="imgs/bmvc2016.png" alt="">
                </a>
            </div>
            <div class="col-md-6">
                <h5>
                 <a href="#">
                Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos, British Machine Vision Conference (BMVC)  2016.    
                </a>
                </h5>
                <h5>
                 <a href="#"> Suman Saha</a>,
                 <a href="http://gurkirt.github.io/">Gurkirt Singh</a>, 
                 <a href="https://sites.google.com/site/mikesapi/">Michael Sapienza</a>,
                 <a href="http://www.robots.ox.ac.uk/~tvg/"> Philip H. S. Torr</a>,
                 <a href="http://cms.brookes.ac.uk/staff/FabioCuzzolin/">Fabio Cuzzolin.</a>
                </h5>
                <p align="justify">                
                In this work we propose a new approach to the spatio-temporal localisation (detection) and classification of multiple concurrent actions within
		temporally untrimmed videos. We demonstrate the performance of our algorithm on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets,
		achieving new state-of-the-art results across the board
		and significantly lower detection latency at test time.			
                </p>
                <a href="bmvc2016/index.html" target="_blank">
                Project page
                </a> |
                <a href="http://arxiv.org/abs/1608.01529" target="_blank">
                arXiv
                </a> |
                <a href="https://drive.google.com/file/d/0B56r0lkehn6WLXdzU2lyZXpINDA/view?usp=sharing" target="_blank">
                Paper+supplementary
                </a> |
                <a href="https://github.io/sahasuman/bmvc2016_code" name="code" target="_blank">Code
                </a> |
                <a href="https://drive.google.com/open?id=0B56r0lkehn6WZHZqdmtiVFhMWUE" target="_blank">Poster
                </a>
            </div>
       </div>
        <!-- /.row -->
        
        <hr>
        
        
        <!-- action detection + instance segmentation ;  submitted to CVPR 2016 but rejected -->
        <div class="row">
            <div class="col-md-6">
                <a href="#">
                    <img class="img-responsive" src="cvpr2016/intro-small.png" alt="">
                </a>
            </div>
            <div class="col-md-6">
                <h5>
                <a href="#">
                Spatio-temporal human action localisation and instance segmentation in temporally untrimmed videos.
                Submitted to https://arxiv.org.                
                
                </a>
                </h5>
                <h5>
                 <a href="#"> Suman Saha</a>,
                 <a href="http://gurkirt.github.io/">Gurkirt Singh</a>, 
                 <a href="https://sites.google.com/site/mikesapi/">Michael Sapienza</a>,
                 <a href="http://www.robots.ox.ac.uk/~tvg/"> Philip H. S. Torr</a>,
                 <a href="http://cms.brookes.ac.uk/staff/FabioCuzzolin/">Fabio Cuzzolin.</a>
                </h5>
                <p align="justify">                
		Current state-of-the-art human action recognition is focused on 
		the classification of temporally trimmed videos in which only one action occurs per frame.
		In this work we address the problem of action localisation and instance segmentation in 
		which multiple concurrent actions of the same class may be segmented out of an image sequence.
		We demonstrate the performance of our algorithm on the challenging LIRIS-HARL 
		dataset and achieve a new state-of-the-art result which is 57.10% better than next best methods.
                </p>                
                <a target="_blank" href="cvpr2016/index.html"> project page </a> | 
                <a href="http://arxiv.org/abs/1707.07213" target="_blank"> arXiv </a>
               
            </div>
       </div>
       
       <hr>
       
        <div class="row">
            <div class="col-md-6">
                <a href="#">
                    <img class="img-responsive" src="Msc2014/intro-small.png" alt="">
                </a>
            </div>
            <div class="col-md-6">
                <h5>
                <a target="_blank" href="https://drive.google.com/open?id=0B56r0lkehn6WQlpwZlFkUjZ5eEE">
                A real-time monocular vision-based frontal obstacle detection and avoidance for low cost UAVs in GPS denied environment, 
                Aerospace Electronics and Remote Sensing Technology (ICARES), 2014 IEEE International Conference on. 
                </a>
                </h5>
                <h5>
                 <a href="#"> Suman Saha</a>,
                 <a href="#"> Ashutosh Natraj </a>, 
                 <a href="#"> Sonia Waharte </a>.                
                </h5>
                <p align="justify">                
		This paper presents a novel monocular vision-based realtime obstacle detection and avoidance for a low cost
                unmanned aerial vehicle (UAV) in an unstructured, GPS denied environment. We propose a mathematical model to estimate the
                relative distance from the UAV’s camera to an obstacle which will subsequently be used in a collision avoidance algorithm.
                We validate our model with some real time experiments under both stationary and dynamic motion of the UAV during its
                flight. The results show good agreement with the ground truth values with an acceptable percentage of error in estimation
                under 3% thus proving it can facilitate obstacle detection and avoidance for low cost and lightweight UAVs.
                </p>
                <a href="https://drive.google.com/open?id=0B56r0lkehn6WQlpwZlFkUjZ5eEE" target="_blank">
                paper | 
                </a>
                <a href="#"> bibtex </a> |
                <a target="#" href="Msc2014/index.html"> project page </a> | 
                <a href="#"> code </a> | 
                <a href="#"> video </a>
                <!-- put here link -->
                <!--<a class="btn btn-primary" href="#">View Project <span class="glyphicon glyphicon-chevron-right"></span></a>-->
            </div>
       </div>
       
       
       <hr>
       
       
       
       <!-- END OF PROJECT SECTION -->
       
       <!-- START OF PUBLICATION SECTION -->
       
        <div class="row"> 
		  <h3><a name="publication">Publications</a></h3>
		 <ul>
		 <li>
		 <a href="#" target="_blank">
		 AMTnet: Action-Micro-Tube regression by end-to-end trainable deep architecture. International Conference on Computer Vision), Venice, Italy, October 22-29, 2017.
		 Saha, Suman, Gurkirt Singh, and Fabio Cuzzolin. 
		 </a>
		 </li>	
		 <li>
		 <a href="#" target="_blank">
		 Online Real-time Multiple Spatiotemporal Action Localisation and Prediction. International Conference on Computer Vision), Venice, Italy, October 22-29, 2017.
		 Saha, Suman, Gurkirt Singh, Michael Sapienza, Philip H. S. Torr, and Fabio Cuzzolin. 
		 </a>
		 </li>
		 
		 <li>	
		 <a href="#" target="_blank">
		 Metric learning for Parkinsonian identification from IMU gait measurements." Gait & Posture 54 (2017): 127-132.
		 <a href="http://cms.brookes.ac.uk/staff/FabioCuzzolin/">Fabio Cuzzolin</a>,              
                 <a href="https://sites.google.com/site/mikesapi/">Michael Sapienza</a>,
                 <a href='#'> Patrick Esser</a>,
                 <a href="#"> Suman Saha</a>,
                 <a href="#"> Miss Marloes Franssen</a>,                  
                 <a href="#"> Johnny Collett </a>,
                 <a href="#"> Helen Dawes </a>.
                 </a>
                 </li>
		 
		 <li>
		 <a href="#" target="_blank">
		 Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos, British Machine Vision Conference (BMVC) 2016.
		 Suman Saha, Gurkirt Singh, Michael Sapienza, Philip H. S. Torr, Fabio Cuzzolin. 
		 </a>
		 </li>
		 <li>
		  <a href="#" target="_blank">
		   Spatio-temporal action tube detection and instance level segmentation of actions, we will submit this to ArXiv. 2016
		   Suman Saha, Gurkirt Singh, Michael Sapienza, Philip H. S. Torr, Fabio Cuzzolin. 
		  </a>
		 </li>
		 <li>
		  <a href="https://drive.google.com/open?id=0B56r0lkehn6WQlpwZlFkUjZ5eEE" target="_blank">
		   A real-time monocular vision-based frontal obstacle detection and avoidance for low cost UAVs in GPS denied environment,
		   Aerospace Electronics and Remote Sensing Technology (ICARES), 2014 IEEE International Conference on. Suman Saha, Ashutosh Natraj , Sonia Waharte. 
		  </a>
		 </li>
		 
		 <li>
		  <a href="http://www.eujast.org/files/VOLUME%201/EUJAST5.pdf" target="_blank">
		  Face Recognition using PCA and Multilayer Feedforward Neural Networks, Suman Saha, European Journal of Applied Sciences and Technology [EUJAST] Volume 1 (1), March 2014.
		  </a>
		 </li>
		 
		 <li>
		  <a href="http://www.ejassr.org/files/March%202014/ARTICLE_4_MARCH_2014.pdf" target="_blank">
		  Parallelization of Genetic Algorithms using MapReduce, Suman Saha, European Journal of Applied Social Sciences Research (EJASSR), Vol-2, Issue 1, March 2014.
		  </a>
		 </li>
		 
		 <li>
		  <a href="https://drive.google.com/open?id=0B56r0lkehn6WWS1GMk8yZTJuQ1U" target="_blank">
		  A Monocular Vision Approach for Obstacle Detection and Collision Avoidance for Low-cost Quadrocopters.
		  MSc thesis, Suman Saha, University of Bedfordshire, UK. January 2014.
		  </a>
		 </li>
		 
		 </ul>
	</div>
	
		  <div class="row"> 
		  <h3><a name="talks">Technical Reports / Talks / Survey Reports </a></h3>
		 <ul>
		 <li>
		 <a href="https://drive.google.com/open?id=0B56r0lkehn6WdnpjWUdnek9oVW8" target="_blank">
		 A survey report on Deep Learning Approach for Human Action Detection from Video 		
		 submitted to the Department of Computing and Communication Technologies, Oxford Brookes University, September, 2015.
		  </a> 
		 </li>
		 
		 <li>
		  <a href="https://drive.google.com/open?id=0B56r0lkehn6WVS00clZTeXVabUE" target="_blank">
		  Performance analysis on temporal tubes: A benchmarking report, Suman Saha,
		  Artificial Intelligence and Vision Research Group, Department of Computing and Communication Technologies, Oxford Brookes University, 2014.
		  </a>
		 </li>
		 
		 <li>
		  <a href="https://drive.google.com/open?id=0B56r0lkehn6WZ0tMYWVObl96Tmc" target="_blank">
		  Streaming hierarchical graph based video segmentation: A step-by-step guide, Suman Saha,
		  Artificial Intelligence and Vision Research Group, Department of Computing and Communication Technologies, Oxford Brookes University, 2014.	
		  </a>
		 </li>
		 
		 <li>
		  <a href="https://drive.google.com/open?id=0B56r0lkehn6WX25DT2NneHBHTDQ" target="_blank">
		  A talk  on "Streaming hierarchical graph based video segmentation (SGBH)"
		  presented at the Robotics research group seminar, Oxford Brookes University, November, 2014.
		  </a>
		 </li>
				 
		 </ul>
		 </div>
       
       <hr>
	  <div class="row"> 
		  <h3>  <a name="awards">Awards</h3>  </a>    
	  <ul>
	  <li><a href="#">Our group won the reading group competition at ICVSS 2015 Summer School.</a></li>
	  <li><a href="https://drive.google.com/open?id=0B56r0lkehn6WdWctSEZ6Sk1nd1E" target="_blank">
	  Received Internation Next 10 Studentship award to support full-time 3 years PhD course in the Department of Computing and Communication Technologies at Oxford Brookes University, UK. 
	  </a></li>
	  <li><a href="https://drive.google.com/open?id=0B56r0lkehn6WLWNYUmpvN0k0M3M" target="_blank">
	  Received Overseas PhD Scholarship award from Department of Computer Science, Aberystwyth University, UK.
	  </a></li>
	  <li><a href="https://drive.google.com/open?id=0B56r0lkehn6WSi1MSzFtbDJWQ3c" target="_blank">
	  Best Overall Performance award for Masters degree course on 3rd April, 2014. 
	  </a></li>
	  <li><a href="https://drive.google.com/open?id=0B56r0lkehn6WUmt4TmlmaW8tZjA" target="_blank">
	  Best Masters Project award on 3rd April, 2014. 
	  </a></li>
	  <li><a href="https://drive.google.com/open?id=0B56r0lkehn6WZ0tqMTd1cF9pa2c" target="_blank">
	  Merit Scholarship award at Masters level.  
	  </a></li>
	 <li><a href="#">Awarded for scoring highest marks in Computers in standard X(96%) and XII(grade: A).</a>
	  </li>
	 <li><a href="#">Ranked 1st in standard 'X' final board examinations.</a>
	  </ul>	  
	 
	  <div class="row"> 
	  <div class="col-md-6"> 	  
                     <iframe width="550" height="315" src="https://www.youtube.com/embed/YYK3Q56uPHM"> </iframe>  
                     <p>Best Overall Performance award for Masters degree course received from Vice Chancellor and Chief Executive
	  <a href="http://www.beds.ac.uk/about-us/our-people/vc-office/our-office/bill-rammell" target="_blank">
	   Bill Rammell
	   </a>
	   , University of Bedfordshire, video taken on 3rd April, 2014.   </p>
            </div>
	  
	  <div class="col-md-6"> 	  
                     <iframe width="550" height="315" src="https://www.youtube.com/embed/O-qOn4f1R58"> </iframe> 
                     <p> Msc dissertation outcome: the proposed obstacle detection and avoidance algorithm is deployed in realtime.</p>
            </div>
	  </div>
       
       
      
        
         <hr>
	  <div class="row"> 
		  <h3>  <a name="activity">Activities</h3>  </a>    
	  <ul>
	  
	       <li>
	       June, 2016: Attended two reading groups in the Department of Engineering Science at University of Oxford; topic discussed 
	       Generative Adversarial Nets (GANs),  Variational Autoencoders (VAEs).
	       </li>
	      <li> February 2016: A talk on 
		<a href="https://drive.google.com/open?id=0B56r0lkehn6WNWNPWF9VRE9FeU0">
		spatio-temporal human action localisation
		</a> 
		presented at CCT Dept. Research Seminar at at Oxford Brookes University. 
		</li>
		<li> January 2016: An introductory lecture on 
		<a href="https://drive.google.com/open?id=0B56r0lkehn6WNWNPWF9VRE9FeU0">
		Advance Computer Vision
		</a>
		delivered to MSc students at Oxford Brookes University. 
		</li>
		<li>
	       6th July 2015: Attended talk on "Visualizing and Understanding Recurrent Neural Networks" by Andrej Karpathy (from Stanford Computer Science group) at Engineering and Science 
	      Department, Oxford University.
	       </li>
		
	</ul>
	
	</div>
        
        
       <div class="row"> 
        <h3>  <a name="robot">Extracurricular Activities </a> at <a href="http://cct.brookes.ac.uk/research/isec/cognitive-robotics/"> Cognitive Robotics Group,
        Oxford Brookes University, UK </a>: 
</h3>
                <p>
		1st December 2015: Actively participated in the <a target="_blank" href="http://cct.brookes.ac.uk/news/items/101215-artie-and-robbie-at-teentech-city.html">
		 TeenTech event </a> at London.<br>
		30th September, 2015: Attended the <a target="_blank" href="http://www.brookes.ac.uk/about-brookes/news/us-ambassador-visits-oxford/">
		 Ambassadors visit at Gipsy Lane campus, Oxford Brookes University, UK.
		</a>
		<br>
		14th September, 2015: Actively participated in the <a target="_blank" href="brookes/bbc-london.jpg"> BBC News live broadcast </a>  BBC broadcasting hub, London. 
		Watch: <a target="_blank" href="https://youtu.be/HGMAvPVHuUs"> video1 </a>, <a target="_blank" href="https://youtu.be/Tb4_4uNzIrY"> video2.</a> <br>
		18th June, 2015: Taking Artie to Magna Carta event on 18th June 2015.<br>
		9th May, 2015:  Actively participated in the <a target="_blank" href="http://cct.brookes.ac.uk/staff/artie.html"> humanoid robot Artie</a>
		and the Naos at the <a href="brookes/a2.png"> Outburst festival </a>.<br>
		12 March, 2015: 2. Actively participated in the <a target="_blank" href="brookes/a1.png">Joint University Alliance and Deloitte research showcase</a>,
		at 17.30-19.00, venue-The Royal Institution, 21 Albemarle Street, London, W1S 4BS, UK. <br>
		31st January, 2015: Attended the Real World Impact filming event organised by
                University Alliance on 31st January, 2015 at Gipsy lane campus, Oxford Brookes University, UK. <br>
		
	      </p>
	 </div>
	 
	<hr>
	
	 <h3>  <a name="gallery"> Photo/Video Gallery </a> </h3>
	 <div class="row">
            <div class="col-md-6">                
                    <img class="img-responsive" src="icvss2015/group-pic.png" alt="">
                    <p> 
                     12th July to 18th July 2015: Attended <a target="_blank" href="http://svg.dmi.unict.it/icvss2015/"> (ICVSS) 2015</a>, held in Sicily, Italy.
                    </p>
            </div>
             <div class="col-md-6" align="center">                
                    <img class="img-responsive" src="icvss2015/poster-day.png" alt="">
                    <p> 
                    12th July to 18th July 2015: Presented a <a target="_blank" href="icvss2015/poster.pdf"> poster </a>
                on "Online Human Action Localisation based on Appearance and Motion Cues" at ICVSS 2015 </a>
                    </p>
             </div> 
         </div>   
         
	<div class="row">
            <div class="col-md-6">                
                    <img class="img-responsive" src="brookes/a3.jpg" alt="">
                    <p> 
                    On 1st December, 2015, PhD student Suman Saha and Department Head Nigel Crook took robots
                    Artie and Robbie to the  <a target="_blank" href="http://cct.brookes.ac.uk/news/items/101215-artie-and-robbie-at-teentech-city.html">
                    TeenTech </a> City event at the Copperbox stadium in London.  
                    </p>
            </div>
             <div class="col-md-6">                
                    <img class="img-responsive" src="brookes/bbc-london.jpg" alt=""> 
                    <p> 
                     14th September, 2015: at the BBC broadcasting hub, London with the Cognitive Robotics Team, Oxford Brookes University, UK.
                    </p>
             </div> 
         </div>    
         <div class="row">  
              <div class="col-md-6">                
                     <iframe width="550" height="315" src="https://www.youtube.com/embed/HGMAvPVHuUs"> </iframe> 
                    <p> 14th September, 2015: at the BBC broadcasting hub, London </p>
                    
            </div>
             <div class="col-md-6">                
                     <iframe width="550" height="315" src="https://www.youtube.com/embed/Tb4_4uNzIrY"> </iframe> 
                    <p> 14th September, 2015: at the BBC broadcasting hub, London </p>
             </div>
            
        </div>
        <div class="row">
            <div class="col-md-6">                
                    <img class="img-responsive" src="brookes/a1.png" alt="">
                    <p> 
                    12 March, 2015: Joint University Alliance and Deloitte research showcase</a>,
		at 17.30-19.00, venue-The Royal Institution, 21 Albemarle Street, London, W1S 4BS, UK.
                    </p>
            </div>
             <div class="col-md-6">                
                    <img class="img-responsive" src="brookes/outburst.png" alt=""> 
                    <p> 
                     9th May, 2015: he <a target="_blank" href="http://cct.brookes.ac.uk/staff/artie.html"> humanoid robot Artie</a>
		and the Naos at the <a href="brookes/a2.png"> Outburst festival </a>.
                    </p>
             </div> 
         </div> 
                 
        <!-- /.row -->
	 
        <!-- Footer -->  
     <br><br><br><br> 
     <hr>
     <hr>
     
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>
                    Webpage designed by <a href="http://getbootstrap.com/2.3.2/index.html"> Bootstrap </a>.
                    </p>
                </div>
            </div>
            <!-- /.row -->
        </footer>

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- visitor counter -->
    <div style="text-align:center;">
    <script type="text/javascript" src="http://services.webestools.com/cpt_visits/13769-10-9.js">
    </script>
    </div>
    
    
</body>

</html>
